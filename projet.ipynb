{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/utilisateur/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/utilisateur/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Import\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "from utils.dataset import BirdDataset\n",
    "from utils.models import VisionTransformer, AutoEncoderLSTM, AutoEncoderAtt\n",
    "from utils.utils import mixup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics as tm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnathan-vidal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/utilisateur/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/utilisateur/createch/tidzam/wandb/run-20230109_145341-wolq7mw5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nathan-vidal/cuitcuit/runs/wolq7mw5\" target=\"_blank\">fearless-vortex-57</a></strong> to <a href=\"https://wandb.ai/nathan-vidal/cuitcuit\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/.local/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# Weight and biases\n",
    "wandb.login(key = '15b11b57c09bdcd801af92cecb362a1f2634d213')\n",
    "wandb.init(project=\"cuitcuit\")\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lumiere(pl.LightningModule):\n",
    "    def __init__(self, model : nn.Module, criterion : nn.Module, num_classes : int, mixup : bool) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.mixup = mixup\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # logger\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # metrics\n",
    "        self.accuracy = tm.Accuracy(task = 'multiclass', num_classes = num_classes)\n",
    "        self.f1 = tm.F1Score(task = 'multiclass', num_classes = num_classes)\n",
    "        self.cf = tm.ConfusionMatrix(task = 'multiclass', num_classes = num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        \n",
    "        if self.mixup :\n",
    "            y = torch.nn.functional.one_hot(y, self.num_classes)\n",
    "            x, y = mixup(x, y, 0.5)\n",
    "        \n",
    "        logits = self.model(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # metrics\n",
    "        self.accuracy(logits, y)\n",
    "        self.f1(logits, y)\n",
    "        \n",
    "        # log metrics\n",
    "        self.log('train_loss', loss, prog_bar=True, logger = True, on_epoch=True)\n",
    "        self.log('train_acc', self.accuracy, prog_bar=True, logger = True, on_epoch=True)\n",
    "        self.log('train_f1', self.f1, prog_bar=True, logger = True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation_step defines the validation loop.\n",
    "        x, y = batch\n",
    "            \n",
    "        \n",
    "        logits = self.model(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # metrics\n",
    "        self.accuracy(logits, y)\n",
    "        self.f1(logits, y)\n",
    "        \n",
    "        # log metrics\n",
    "        self.log('valid_loss', loss, prog_bar=True, logger = True, on_epoch=True)\n",
    "        self.log('valid_acc', self.accuracy, prog_bar=True, logger = True, on_epoch=True)\n",
    "        self.log('valid_f1', self.f1, prog_bar=True, logger = True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test_step defines the test loop.\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # metrics\n",
    "        self.accuracy(logits, y)\n",
    "        self.f1(logits, y)\n",
    "        self.cf(logits, y)\n",
    "        \n",
    "        # log metrics\n",
    "        self.log('test_loss', loss, prog_bar=True, logger = True, on_epoch=True)\n",
    "        self.log('test_acc', self.accuracy, prog_bar=True, logger = True, on_epoch=True)\n",
    "        self.log('test_f1', self.f1, prog_bar=True, logger = True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 100, eta_min = 1e-6)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'houwre': 0, 'houspa': 1, 'comrav': 2, 'eursta': 3, 'redcro': 4, 'gbwwre1': 5, 'sonspa': 6, 'spotow': 7, 'barswa': 8, 'norcar': 9}\n",
      "torch.Size([3, 128, 313])\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "num_classes = 10\n",
    "csv = pd.read_csv(\"dataset/spec.csv\")\n",
    "\n",
    "counts = csv['en'].value_counts()\n",
    "chosen = counts[counts>=100].head(num_classes).index\n",
    "csv = csv[csv['en'].isin(chosen)]\n",
    "\n",
    "csv = csv.sample(frac = 1).reset_index(drop = True)\n",
    "class_mapping = {bird : i for i, bird in enumerate(chosen)}\n",
    "print(class_mapping)\n",
    "train = csv.iloc[:int(len(csv)*0.8)]\n",
    "valid = csv.iloc[int(len(csv)*0.8):int(len(csv)*0.9)]\n",
    "test = csv.iloc[int(len(csv)*0.9):]\n",
    "\n",
    "train_dataset = BirdDataset(train, \"dataset/spectrograms/\", class_mapping)\n",
    "print(train_dataset[0][0].shape)\n",
    "valid_dataset = BirdDataset(valid, \"dataset/spectrograms/\", class_mapping)\n",
    "test_dataset = BirdDataset(test, \"dataset/spectrograms/\", class_mapping)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, num_workers = 4, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 16, shuffle = False, num_workers = 4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False, num_workers = 4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 128, 313])\n"
     ]
    }
   ],
   "source": [
    "examples = next(iter(train_loader))\n",
    "print(examples[0].shape)\n",
    "# for label, img  in enumerate(examples):\n",
    "#    plt.imshow(img[0,0,:,:])\n",
    "#    plt.show()\n",
    "#    print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model AutoEncoder using LSTM\n",
    "modellstm = efficientnet_b0()\n",
    "modellstm.classifier = nn.Identity()\n",
    "aelstm = AutoEncoderLSTM(modellstm, 128, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model AutoEncoder using TransformerDecoder ie Attention\n",
    "modelatt = efficientnet_b0()\n",
    "modelatt.classifier = nn.Identity()\n",
    "aeatt = AutoEncoderAtt(modelatt, 1280, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n",
      "/home/utilisateur/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'criterion' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['criterion'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "mixup_bool = True\n",
    "model = efficientnet_b0()\n",
    "model.classifier = nn.Sequential(\n",
    "          nn.Dropout(p=0.2, inplace=True),\n",
    "          nn.Linear(in_features=model.classifier[1].in_features, out_features = num_classes, bias = True)\n",
    "      )\n",
    "if mixup_bool :\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else :\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "model_pl = Lumiere(aeatt, criterion, len(chosen), mixup_bool)\n",
    "callbacks = [ModelCheckpoint(dirpath = 'save/lstm', monitor = 'valid_loss', save_top_k = 1, mode = 'min'), \n",
    "             EarlyStopping(monitor = 'valid_loss', patience = 2, mode = 'min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "/home/utilisateur/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/utilisateur/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "trainer = Trainer(accelerator = 'gpu',\n",
    "                  devices = 1,\n",
    "                  max_epochs = 30,\n",
    "                  logger = wandb_logger,\n",
    "                  callbacks= callbacks,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /home/utilisateur/createch/tidzam/save/lstm exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                      | Params\n",
      "--------------------------------------------------------\n",
      "0 | model     | AutoEncoderAtt            | 114 M \n",
      "1 | criterion | CrossEntropyLoss          | 0     \n",
      "2 | accuracy  | MulticlassAccuracy        | 0     \n",
      "3 | f1        | MulticlassF1Score         | 0     \n",
      "4 | cf        | MulticlassConfusionMatrix | 0     \n",
      "--------------------------------------------------------\n",
      "114 M     Trainable params\n",
      "0         Non-trainable params\n",
      "114 M     Total params\n",
      "456.993   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1175/1175 [01:55<00:00, 10.17it/s, loss=2.3, v_num=7mw5, train_loss_step=2.260, train_acc_step=0.0833, train_f1_step=0.0833, valid_loss=2.340, valid_acc=0.106, valid_f1=0.106, train_loss_epoch=2.440, train_acc_epoch=0.000, train_f1_epoch=0.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: The ``compute`` method of metric MulticlassAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/utilisateur/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: The ``compute`` method of metric MulticlassF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1175/1175 [01:56<00:00, 10.11it/s, loss=2.29, v_num=7mw5, train_loss_step=2.270, train_acc_step=0.0833, train_f1_step=0.0833, valid_loss=2.300, valid_acc=0.113, valid_f1=0.113, train_loss_epoch=2.300, train_acc_epoch=0.000, train_f1_epoch=0.000]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model = model_pl, \n",
    "             train_dataloaders = train_loader, \n",
    "             val_dataloaders = valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 131/131 [00:02<00:00, 51.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10823754966259003    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10823754966259003    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.2984979152679443     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10823754966259003   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10823754966259003   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.2984979152679443    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.2984979152679443,\n",
       "  'test_acc': 0.10823754966259003,\n",
       "  'test_f1': 0.10823754966259003}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model_pl, dataloaders = test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
